
<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <title>Versatile Multimodal Controls for Expressive Talking Human Animation</title>
        <link rel="stylesheet" href="static/css/bulma.min.css">
        <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
        <link rel="stylesheet" href="static/css/bulma-slider.min.css">
        <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        <link href="./asserts/style.css" rel="stylesheet">

        <!-- 导航栏相关CSS -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
        
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    </head>

    <body>

        <!-- 导航栏代码 -->
        <nav class="navbar" role="navigation" aria-label="main navigation">
            <div class="navbar-brand">
                <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                </a>
            </div>
            <div class="navbar-menu">
                <div class="navbar-start" style="flex-grow: 1; justify-content: center;">

                    <div class="navbar-item has-dropdown is-hoverable">
                        <a class="navbar-link">
                            More Related Research
                        </a>
                        <div class="navbar-dropdown">
                            <a class="navbar-item" href="https://digital-avatar.github.io/ai/Ditto/" target="_blank">
                                Ditto-Talkinghead
                            </a>
                            <a class="navbar-item" href="https://digital-avatar.github.io/ai/LokiTalk/" target="_blank">
                                LokiTalk
                            </a>
                            <a class="navbar-item" href="https://digital-avatar.github.io/ai/VersaAnimator/" target="_blank">
                            VersaAnimator
                            </a>
                            <a class="navbar-item" href="https://digital-avatar.github.io/ai/HumanSense/" target="_blank">
                            HumanSense
                            </a>
                        </div>
                    </div>
                </div>

            </div>
        </nav>

        <!-- JavaScript 控制汉堡菜单 -->
        <script>
            document.addEventListener('DOMContentLoaded', function () {
            const burger = document.querySelector('.navbar-burger');
            const menu = document.querySelector('.navbar-menu');

            if (burger && menu) {
                burger.addEventListener('click', () => {
                burger.classList.toggle('is-active');
                menu.classList.toggle('is-active');
                });
            }
            });
        </script>

        <div class="content">
            <h1><strong> VersaAnimator:<br> Versatile Multimodal Controls for Expressive Talking Human Animation </strong></h1>

            <p id="authors" class="serif">
                <a>Zheng Qin<sup>#</sup><sup>1</sup></a>,
                <a>Ruobing Zheng<sup>#</sup><sup>*</sup><sup>2</sup></a>,
                <a>Yabing Wang<sup>1</sup></a>,
                <a>Tianqi Li<sup>2</sup></a>,
                <br>
                <a>Zixin Zhu<sup>3</sup></a>,
                <a>Sanping Zhou<sup>1</sup></a>,
                <a>Ming Yang<sup>2</sup></a>,
                <a>Le Wang<sup>†<dag><sup>1</sup></a> <br>

                <span style="font-size: 18px; margin-top: 0.8em">
                    <br>
                    <sup>†</sup>Corresponding Author.
                    <br>
                    <sup>*</sup>Project Lead.
                    <br>
                    <sup>#</sup>Co-first authors.
                    <br>
                    <sup>1</sup>Xi’an Jiaotong University. <sup>2</sup>Ant Group. <sup>3</sup>University at Buffalo.
                    <br>
                </span>
            </p>

            
            <div class="column-flex">
                <div class="flex flex-gap" style="margin-bottom:0.5em;">
                    <a target="_blank" href="https://arxiv.org/abs/2503.08714" ><button><i class="ai ai-arxiv"></i> Paper</button></a>
                    <a target="_blank" href=""><button><i class="fa fa-github"></i> Code (coming soon)</button></a>
                </div>
            </div>

            <br>

            <div id="teasers">
                <img src="./asserts/1.png", style="width: 100%;">
                <figcaption></figcaption>
            </div>

            <br>


        </div>

        <div class="content">
            <h2 style="text-align:center"><strong>Abstract</strong></h2>

            <div id="teasers">
                <img src="./asserts/2.png", style="width: 100%;">
                <figcaption></figcaption>
            </div>

            <p style="line-height: 30px; font-size: 18px;">
                In filmmaking, directors typically allow actors to perform freely based on the script before providing specific guidance on how to present key actions. AI-generated content faces similar requirements, where users not only need automatic generation of lip synchronization and basic gestures from audio input but also desire semantically accurate and expressive body movement that can be ``directly guided'' through text descriptions. Therefore,  we present VersaAnimator, a versatile framework that synthesizes expressive talking human videos from arbitrary portrait images. Specifically, we design a motion generator that produces basic rhythmic movements from audio input and supports text-prompt control for specific actions. The generated whole-body 3D motion tokens can animate portraits of various scales, producing talking heads, half-body gestures and even leg movements for whole-body images. Besides, we introduce a multi-modal controlled video diffusion that generates photorealistic videos, where speech signals govern lip synchronization, facial expressions, and head motions while body movements are guided by the 2D poses. Furthermore, we introduce a token2pose translator to smoothly map 3D motion tokens to 2D pose sequences. This design mitigates the stiffness resulting from direct 3D to 2D conversion and enhances the details of the generated body movements. Extensive experiments shows that VersaAnimator synthesizes lip-synced and identity-preserving videos while generating expressive and semantically meaningful whole-body motions.
            </p>

        </div>


        
        <div class="content">
            <h2 style="text-align: center;"><strong>Gallery</strong></h2>

            <h3>Visual illustration of text control for customizing the character’s motion in the generated video</h3>
            <div class="gallery">
                <div class="row">
                    <img src="./asserts/3.png", style="width: 100%;">
                </div>
            </div>
            
            <br>

            <h3>Results on Multi-Animate with different audio and reference images ranging from head to whole-body.</h3>
            <div class="gallery">
                <div class="row">
                    <img src="./asserts/4.png", style="width: 100%;">
                </div>
            </div>
        </div>

       

        <div class="content">
            <h2 style="text-align:center; margin: 0 auto;"><strong>BibTex</strong></h2>
            <div class="bibtex" style="font-size: 15px;">
            
                <code>
                    @article{qin2025versatile,<br>
                    &nbsp; title={Versatile Multimodal Controls for Expressive Talking Human Animation},<br>
                    &nbsp; author={Qin, Zheng and Zheng, Ruobing and Wang, Yabing and Li, Tianqi and Zhu, Zixin and Zhou, Sanping and Yang, Ming and Wang, Le},<br>
                    &nbsp; journal={arXiv preprint arXiv:2503.08714},<br>
                    &nbsp; year={2025}<br>
                    }
                </code>

            </div>
        </div>

        <footer style="text-align: center; font-size: medium; color: blueviolet;">
            <span id="busuanzi_container_page_pv">Page Views: <span id="busuanzi_value_page_pv"></span></span>
        </footer>

    </body>

</html>
