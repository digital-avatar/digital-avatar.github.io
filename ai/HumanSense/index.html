

<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <title>HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs</title>
        <link rel="stylesheet" href="static/css/bulma.min.css">
        <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
        <link rel="stylesheet" href="static/css/bulma-slider.min.css">
        <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        <link href="./asserts/style.css" rel="stylesheet">

        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    </head>

    

    <body>

        <div class="content">

            <div id="teasers">
                <img src="./asserts/figure1.png", style="width: 100%;">
                <figcaption></figcaption>
            </div>
            
            <h1><strong> HumanSense:<br> From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs </strong></h1>

            <p id="authors" class="serif">
                <a href="https://scholar.google.com/citations?user=sPQqpXsAAAAJ&hl=en&oi=sra">Zheng Qin<sup>1</sup></a>,
                <a href="https://scholar.google.com/citations?user=S8FmqTUAAAAJ&hl=en">Ruobing Zheng<sup>*</sup><sup>2</sup></a>,
                <a href="https://scholar.google.com/citations?user=3WVFdMUAAAAJ&hl=en">Yabing Wang<sup>1</sup></a>,
                <a href="https://scholar.google.com/citations?user=yOtsVWQAAAAJ&hl=en&oi=sra">Tianqi Li<sup>2</sup></a>,
                <br>
                <a href="https://yuanyi.pub/">Yi Yuan<sup>2</sup></a>,
                <a href="https://scholar.google.com/citations?hl=en&user=8SCEv-YAAAAJ&view_op=list_works&sortby=pubdate">Jingdong Chen<sup>2</sup></a>,
                <a href="https://scholar.google.com/citations?user=RypRCUQAAAAJ&hl=en">Le Wang<sup>†<dag><sup>1</sup></a> <br>

                <span style="font-size: 18px; margin-top: 0.8em">
                    <br>
                    <sup>*</sup>Co-first authors. Project Lead.
                    <br>
                    <sup>†</sup>Corresponding Author.
                    <br>
                    <sup>1</sup>Xi’an Jiaotong University. <sup>2</sup>Ant Group.
                    <br>
                </span>
            </p>

            
            <div class="column-flex">
                <div class="flex flex-gap" style="margin-bottom:0.5em;">
                    <a target="_blank" href="https://arxiv.org/abs/2508.10576" ><button><i class="ai ai-arxiv"></i> Paper</button></a>
                    <a target="_blank" href=""><button><i class="fa fa-github"></i> Code (coming soon)</button></a>
                    <a target="_blank" href="">
                      <button>
                        <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" 
                             alt="Hugging Face" style="height:1em; vertical-align:middle;">
                        Hugging Face (coming soon)
                      </button>
                    </a>
                </div>
            </div>

            <br>
                    
<!--             <h3>Demonstration of tasks in HumanSense.</h3>
            <div id="teasers">
                <img src="./asserts/figure1.png", style="width: 100%;">
                <figcaption></figcaption>
            </div> -->

            <br>


        </div>

        <div class="content">
            <h2 style="text-align:center"><strong>Abstract</strong></h2>

            <p style="line-height: 30px; font-size: 18px;">
            While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. 
            Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. 
            Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks.
            Furthermore, we argue that appropriate feedback stems from a contextual analysis of the interlocutor's needs and emotions, with reasoning ability serving as the key to unlocking it. Accordingly, we employ a multi-stage, modality-progressive reinforcement learning to enhance the reasoning abilities of an Omni model, achieving substantial gains on evaluation results. Additionally, we observe that successful reasoning processes exhibit highly consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner.     
            </p>

        </div>


        
        <div class="content" style="text-align: center;">
<!--             <h2 style="text-align: center;"><strong>Gallery</strong></h2> -->

        
            <div class="gallery">
                <div class="row">
                    <img src="./asserts/figure2.png", style="width: 100%;">
                </div>
            </div>
            <h3>Left: The structure and counts of evaluation tasks in HumanSense. Right: Benchmark statistics on video length.</h3>
            
            <br>

             <div class="gallery">
                <div class="row">
                    <img src="./asserts/table1.png", style="width: 100%;">
                </div>
            </div>
            <h3>Evaluation on HumanSense. † Indicates results on the HumanSense~(tiny) set, for comparison with human-level performance. While GPT-4o is designed as an omni-modal, the absence of audio input support in its current API precluded its evaluation on audio-related tasks~(FR2, PC). For a fair comparison across all models, the overall average scores~(Avg.) for omni-modals are calculated without the results from the two audio tasks. The best results for each metric are bolded.</h3>
           

            
<!--             <div class="gallery">
                <div class="row">
                    <img src="./asserts/figure3.png", style="width: 50%;"  class="center-img">
                </div>
            </div> -->
            <div class="gallery">
              <div class="row" style="text-align:center;">
                <img src="./asserts/figure3.png" style="width:50%;" class="center-img">
              </div>
            </div>
            <h3>Performance Radar Charts on HumanSense (mini). The results include human-level performance with several state-of-the-art multimodal models.</h3>

            <div class="gallery">
                <div class="row">
                    <img src="./asserts/figure5.png", style="width: 100%;">
                </div>
            </div>
            <h3>Examples of Successful Reasoning. These cases cover four high-level perception and interaction tasks, including both video-based and audio-based questions. The reasoning processes all demonstrate thinking that integrates characteristics, emotions, and context, and then provides appropriate feedback.</h3>
            

        
        </div>

       

        <div class="content">
            <h2 style="text-align:center; margin: 0 auto;"><strong>BibTex</strong></h2>
            <div class="bibtex" style="font-size: 15px;">
            
                <code>
                    @article{li2024lokitalk,<br>
                    &nbsp; title={HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs},<br>
                    &nbsp; author={Qin, Zheng and Zheng, Ruobing and Wang, Yabing and Li, Tianqi and Yuan, Yi and Chen, Jingdong and Wang, Le},<br>
                    &nbsp; journal={arXiv preprint arXiv:2508.10576},<br>
                    &nbsp; year={2025}<br>
                    }
                </code>

            </div>
        </div>

        <footer style="text-align: center; font-size: medium; color: blueviolet;">
            <span id="busuanzi_container_page_pv">Page Views: <span id="busuanzi_value_page_pv"></span></span>
        </footer>

    </body>

</html>
